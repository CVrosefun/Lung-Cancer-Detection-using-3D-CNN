{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import dicom\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.ndimage\n",
    "import cv2\n",
    "\n",
    "INPUT_FOLDER = 'sample_images (1)/'\n",
    "patients = os.listdir(INPUT_FOLDER)\n",
    "labels_df = pd.read_csv('stage1_labels.csv', index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_new = np.zeros(20)\n",
    "for i in range(len(patients)):\n",
    "    try:\n",
    "        label= labels_df.get_value(patients[i], 'cancer')\n",
    "        \n",
    "    except KeyError as e:\n",
    "        label = 0\n",
    "    \n",
    "    label_new[i] = label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load dicom files and add slice thickness\n",
    "\n",
    "def load_slices(path):\n",
    "    slices = [dicom.read_file(path + '/' + s) for s in os.listdir(path)]\n",
    "    slices.sort(key = lambda x: int(x.ImagePositionPatient[2]))\n",
    "    try: \n",
    "        slice_thickness = np.abs(slices[0].ImagePositionPatient[2] - slices[1].ImagePositionPatient[2])\n",
    "    except:\n",
    "        slice_thickness = np.abs(slices[0].SliceLocation - slices[1].SliceLocation)\n",
    "    \n",
    "    slices1 = []\n",
    "    \n",
    "    for s in slices:\n",
    "        s.SliceThickness = slice_thickness\n",
    "    \n",
    "    return slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pixels_hu(slices):\n",
    "    image = np.stack([s.pixel_array for s in slices])\n",
    "    image = image.astype(np.int16)\n",
    "\n",
    "    image[ image == -2000] = 0\n",
    "\n",
    "    for slice_number in range(len(slices)):\n",
    "        intercept = slices[slice_number].RescaleIntercept\n",
    "        slope = slices[slice_number].RescaleSlope\n",
    "    \n",
    "        if slope!=1:\n",
    "            image[slice_number] = slope*image[slice_number].astype(np.float64)\n",
    "            image[slice_number] = image[slice_number].astype(np.int16)\n",
    "            image[slice_number] += np.int16(intercept)\n",
    "        \n",
    "    \n",
    "    return np.array(image, dtype = np.int16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def resample(image, scan, new_spacing):\n",
    "    spacing = np.array([scan[0].SliceThickness] + scan[0].PixelSpacing, dtype=np.float32)\n",
    "    resize_factor = spacing / new_spacing\n",
    "    new_real_shape = image.shape * resize_factor\n",
    "    new_shape = np.round(new_real_shape)\n",
    "    real_resize_factor = new_shape / image.shape\n",
    "    new_spacing = spacing / real_resize_factor\n",
    "    \n",
    "    image = scipy.ndimage.interpolation.zoom(image, real_resize_factor, mode = 'nearest')\n",
    "    \n",
    "    return image, new_spacing\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MIN_BOUND = -1000.0\n",
    "MAX_BOUND = 400.0\n",
    "    \n",
    "def normalize(image):\n",
    "    image = (image - MIN_BOUND) / (MAX_BOUND - MIN_BOUND)\n",
    "    image[image>1] = 1.\n",
    "    image[image<0] = 0.\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PIXEL_MEAN = 0.25\n",
    "\n",
    "def zero_center(image):\n",
    "    image = image - PIXEL_MEAN\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "slices = load_slices(INPUT_FOLDER + patients[10])\n",
    "image = get_pixels_hu(slices)\n",
    "image, spacing = resample(image, slices, [1,1,1])\n",
    "\n",
    "slice2 = []\n",
    "for sliceee in image:\n",
    "    slice1 = cv2.resize(sliceee, (25,25))\n",
    "    slice2.append(slice1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 25, 25)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "HM_SLICES = 20\n",
    "new_slices = []\n",
    "\n",
    "image = np.stack([s for s in slice2])\n",
    "\n",
    "np.shape(image)\n",
    "\n",
    "def mean(l):\n",
    "    return sum(l) / len(l)\n",
    "\n",
    "def chunks(l, n):\n",
    "    # Credit: Ned Batchelder\n",
    "    # Link: http://stackoverflow.com/questions/312443/how-do-you-split-a-list-into-evenly-sized-chunks\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]\n",
    "\n",
    "chunk_sizes = math.ceil(len(slice2) / HM_SLICES)\n",
    "for slice_chunk in chunks(slice2, chunk_sizes):\n",
    "    slice_chunk = list(map(mean, zip(*slice_chunk)))\n",
    "    new_slices.append(slice_chunk)\n",
    "\n",
    "np.shape(new_slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "much_data = []\n",
    "new_label = np.zeros(20)\n",
    "for i in range(20):\n",
    "    \n",
    "    try:\n",
    "        label= labels_df.get_value(patients[i], 'cancer')\n",
    "        \n",
    "    except KeyError as e:\n",
    "        label = 0\n",
    "    new_label[i] = label\n",
    "    \n",
    "    slices = load_slices(INPUT_FOLDER + patients[i])\n",
    "    image = get_pixels_hu(slices)\n",
    "    image, spacing = resample(image, slices, [1,11,1])\n",
    "    slice2 = []\n",
    "    new_slices = []\n",
    "    \n",
    "    for sliceee in image:\n",
    "        slice1 = cv2.resize(sliceee, (50,50))\n",
    "        slice2.append(slice1)\n",
    "        \n",
    "    chunk_sizes = math.ceil(len(slice2) / HM_SLICES)\n",
    "    for slice_chunk in chunks(slice2, chunk_sizes):\n",
    "        slice_chunk = list(map(mean, zip(*slice_chunk)))\n",
    "        new_slices.append(slice_chunk)\n",
    "    \n",
    "    if len(new_slices) == HM_SLICES-1:\n",
    "        new_slices.append(new_slices[-1])\n",
    "\n",
    "    if len(new_slices) == HM_SLICES-2:\n",
    "        new_slices.append(new_slices[-1])\n",
    "        new_slices.append(new_slices[-1])\n",
    "\n",
    "    if len(new_slices) == HM_SLICES+2:\n",
    "        new_val = list(map(mean, zip(*[new_slices[HM_SLICES-1],new_slices[HM_SLICES],])))\n",
    "        del new_slices[HM_SLICES]\n",
    "        new_slices[HM_SLICES-1] = new_val\n",
    "        \n",
    "    if len(new_slices) == HM_SLICES+1:\n",
    "        new_val = list(map(mean, zip(*[new_slices[HM_SLICES-1],new_slices[HM_SLICES],])))\n",
    "        del new_slices[HM_SLICES]\n",
    "        new_slices[HM_SLICES-1] = new_val\n",
    "        \n",
    "    \n",
    "    image = np.stack([s for s in new_slices])\n",
    "    \n",
    "    image = normalize(image)\n",
    "    image = zero_center(image)\n",
    "    \n",
    "    much_data.append([image, label])\n",
    "     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 50, 50)\n",
      "(20, 50, 50)\n",
      "(20, 50, 50)\n",
      "(20, 50, 50)\n",
      "(20, 50, 50)\n",
      "(20, 50, 50)\n",
      "(20, 50, 50)\n",
      "(20, 50, 50)\n",
      "(20, 50, 50)\n",
      "(20, 50, 50)\n",
      "(20, 50, 50)\n",
      "(20, 50, 50)\n",
      "(20, 50, 50)\n",
      "(20, 50, 50)\n",
      "(20, 50, 50)\n",
      "(20, 50, 50)\n",
      "(20, 50, 50)\n",
      "(20, 50, 50)\n",
      "(20, 50, 50)\n",
      "(20, 50, 50)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(20):\n",
    "    print(np.shape(much_data[i][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.placeholder('float')\n",
    "y = tf.placeholder('float')\n",
    "n_classes = 2\n",
    "batch_size = 10\n",
    "keep_rate = 0.8\n",
    "\n",
    "def conv3d(x, W):\n",
    "    return tf.nn.conv3d(x, W, strides = [1,1,1,1,1], padding = 'SAME')\n",
    "\n",
    "def maxpool3d(x):\n",
    "    return tf.nn.max_pool3d(x, ksize = [1,2,2,2,1], strides = [1,2,2,2,1], padding = 'SAME')\n",
    "\n",
    "def convolutional_3d(x):\n",
    "    \n",
    "    weights = {'W_conv1':tf.Variable(tf.truncated_normal([3,3,3,1,32], stddev = 0.1)),\n",
    "               'W_conv2':tf.Variable(tf.truncated_normal([3,3,3,32,64], stddev = 0.1)),\n",
    "               'W_fc':tf.Variable(tf.truncated_normal([54080,1024], stddev = 0.1)),\n",
    "               'out':tf.Variable(tf.truncated_normal([1024, n_classes], stddev = 0.1))}\n",
    "    \n",
    "    biases = { 'b_conv1':tf.Variable(tf.constant(0.1, shape = [32])),\n",
    "               'b_conv2':tf.Variable(tf.constant(0.1, shape = [64])),\n",
    "               'b_fc':tf.Variable(tf.constant(0.1, shape = [1024])),\n",
    "               'out':tf.Variable(tf.constant(0.1, shape = [n_classes]))}\n",
    "    \n",
    "    x = tf.reshape(x, shape=[-1, 50, 50, 20, 1])\n",
    "\n",
    "    conv1 = tf.nn.relu(conv3d(x, weights['W_conv1']) + biases['b_conv1'])\n",
    "    conv1 = maxpool3d(conv1)\n",
    "    \n",
    "    conv2 = tf.nn.relu(conv3d(conv1, weights['W_conv2']) + biases['b_conv2'])\n",
    "    conv2 = maxpool3d(conv2)\n",
    "\n",
    "    fc = tf.reshape(conv2,[-1, 54080])\n",
    "    fc = tf.nn.relu(tf.matmul(fc, weights['W_fc'])+biases['b_fc'])\n",
    "    #fc = tf.nn.dropout(fc, keep_rate)\n",
    "\n",
    "    output = tf.matmul(fc, weights['out'])+biases['out']\n",
    "\n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable, check your graph for ops that do not support gradients, between variables ['Tensor(\"Variable/read:0\", shape=(3, 3, 3, 1, 32), dtype=float32)', 'Tensor(\"Variable_1/read:0\", shape=(3, 3, 3, 32, 64), dtype=float32)', 'Tensor(\"Variable_2/read:0\", shape=(8800, 1024), dtype=float32)', 'Tensor(\"Variable_3/read:0\", shape=(1024, 2), dtype=float32)', 'Tensor(\"Variable_4/read:0\", shape=(32,), dtype=float32)', 'Tensor(\"Variable_5/read:0\", shape=(64,), dtype=float32)', 'Tensor(\"Variable_6/read:0\", shape=(1024,), dtype=float32)', 'Tensor(\"Variable_7/read:0\", shape=(2,), dtype=float32)', 'Tensor(\"Variable_8/read:0\", shape=(3, 3, 3, 1, 32), dtype=float32)', 'Tensor(\"Variable_9/read:0\", shape=(3, 3, 3, 32, 64), dtype=float32)', 'Tensor(\"Variable_10/read:0\", shape=(54080, 1024), dtype=float32)', 'Tensor(\"Variable_11/read:0\", shape=(1024, 2), dtype=float32)', 'Tensor(\"Variable_12/read:0\", shape=(32,), dtype=float32)', 'Tensor(\"Variable_13/read:0\", shape=(64,), dtype=float32)', 'Tensor(\"Variable_14/read:0\", shape=(1024,), dtype=float32)', 'Tensor(\"Variable_15/read:0\", shape=(2,), dtype=float32)', 'Tensor(\"Variable_16/read:0\", shape=(3, 3, 3, 1, 32), dtype=float32)', 'Tensor(\"Variable_17/read:0\", shape=(3, 3, 3, 32, 64), dtype=float32)', 'Tensor(\"Variable_18/read:0\", shape=(54080, 1024), dtype=float32)', 'Tensor(\"Variable_19/read:0\", shape=(1024, 2), dtype=float32)', 'Tensor(\"Variable_20/read:0\", shape=(32,), dtype=float32)', 'Tensor(\"Variable_21/read:0\", shape=(64,), dtype=float32)', 'Tensor(\"Variable_22/read:0\", shape=(1024,), dtype=float32)', 'Tensor(\"Variable_23/read:0\", shape=(2,), dtype=float32)', 'Tensor(\"Variable_24/read:0\", shape=(3, 3, 3, 1, 32), dtype=float32)', 'Tensor(\"Variable_25/read:0\", shape=(3, 3, 3, 32, 64), dtype=float32)', 'Tensor(\"Variable_26/read:0\", shape=(54080, 1024), dtype=float32)', 'Tensor(\"Variable_27/read:0\", shape=(1024, 2), dtype=float32)', 'Tensor(\"Variable_28/read:0\", shape=(32,), dtype=float32)', 'Tensor(\"Variable_29/read:0\", shape=(64,), dtype=float32)', 'Tensor(\"Variable_30/read:0\", shape=(1024,), dtype=float32)', 'Tensor(\"Variable_31/read:0\", shape=(2,), dtype=float32)', 'Tensor(\"Variable_32/read:0\", shape=(3, 3, 3, 1, 32), dtype=float32)', 'Tensor(\"Variable_33/read:0\", shape=(3, 3, 3, 32, 64), dtype=float32)', 'Tensor(\"Variable_34/read:0\", shape=(54080, 1024), dtype=float32)', 'Tensor(\"Variable_35/read:0\", shape=(1024, 2), dtype=float32)', 'Tensor(\"Variable_36/read:0\", shape=(32,), dtype=float32)', 'Tensor(\"Variable_37/read:0\", shape=(64,), dtype=float32)', 'Tensor(\"Variable_38/read:0\", shape=(1024,), dtype=float32)', 'Tensor(\"Variable_39/read:0\", shape=(2,), dtype=float32)'] and loss Tensor(\"Mean_2:0\", dtype=float32).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-131-4883b0097a17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mtrain_neural_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-131-4883b0097a17>\u001b[0m in \u001b[0;36mtrain_neural_network\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax_cross_entropy_with_logits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mcorrect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Rejin Joy\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\u001b[0m\n\u001b[1;32m    293\u001b[0m           \u001b[1;34m\"No gradients provided for any variable, check your graph for ops\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m           \u001b[1;34m\" that do not support gradients, between variables %s and loss %s.\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m           ([str(v) for _, v in grads_and_vars], loss))\n\u001b[0m\u001b[1;32m    296\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     return self.apply_gradients(grads_and_vars, global_step=global_step,\n",
      "\u001b[0;31mValueError\u001b[0m: No gradients provided for any variable, check your graph for ops that do not support gradients, between variables ['Tensor(\"Variable/read:0\", shape=(3, 3, 3, 1, 32), dtype=float32)', 'Tensor(\"Variable_1/read:0\", shape=(3, 3, 3, 32, 64), dtype=float32)', 'Tensor(\"Variable_2/read:0\", shape=(8800, 1024), dtype=float32)', 'Tensor(\"Variable_3/read:0\", shape=(1024, 2), dtype=float32)', 'Tensor(\"Variable_4/read:0\", shape=(32,), dtype=float32)', 'Tensor(\"Variable_5/read:0\", shape=(64,), dtype=float32)', 'Tensor(\"Variable_6/read:0\", shape=(1024,), dtype=float32)', 'Tensor(\"Variable_7/read:0\", shape=(2,), dtype=float32)', 'Tensor(\"Variable_8/read:0\", shape=(3, 3, 3, 1, 32), dtype=float32)', 'Tensor(\"Variable_9/read:0\", shape=(3, 3, 3, 32, 64), dtype=float32)', 'Tensor(\"Variable_10/read:0\", shape=(54080, 1024), dtype=float32)', 'Tensor(\"Variable_11/read:0\", shape=(1024, 2), dtype=float32)', 'Tensor(\"Variable_12/read:0\", shape=(32,), dtype=float32)', 'Tensor(\"Variable_13/read:0\", shape=(64,), dtype=float32)', 'Tensor(\"Variable_14/read:0\", shape=(1024,), dtype=float32)', 'Tensor(\"Variable_15/read:0\", shape=(2,), dtype=float32)', 'Tensor(\"Variable_16/read:0\", shape=(3, 3, 3, 1, 32), dtype=float32)', 'Tensor(\"Variable_17/read:0\", shape=(3, 3, 3, 32, 64), dtype=float32)', 'Tensor(\"Variable_18/read:0\", shape=(54080, 1024), dtype=float32)', 'Tensor(\"Variable_19/read:0\", shape=(1024, 2), dtype=float32)', 'Tensor(\"Variable_20/read:0\", shape=(32,), dtype=float32)', 'Tensor(\"Variable_21/read:0\", shape=(64,), dtype=float32)', 'Tensor(\"Variable_22/read:0\", shape=(1024,), dtype=float32)', 'Tensor(\"Variable_23/read:0\", shape=(2,), dtype=float32)', 'Tensor(\"Variable_24/read:0\", shape=(3, 3, 3, 1, 32), dtype=float32)', 'Tensor(\"Variable_25/read:0\", shape=(3, 3, 3, 32, 64), dtype=float32)', 'Tensor(\"Variable_26/read:0\", shape=(54080, 1024), dtype=float32)', 'Tensor(\"Variable_27/read:0\", shape=(1024, 2), dtype=float32)', 'Tensor(\"Variable_28/read:0\", shape=(32,), dtype=float32)', 'Tensor(\"Variable_29/read:0\", shape=(64,), dtype=float32)', 'Tensor(\"Variable_30/read:0\", shape=(1024,), dtype=float32)', 'Tensor(\"Variable_31/read:0\", shape=(2,), dtype=float32)', 'Tensor(\"Variable_32/read:0\", shape=(3, 3, 3, 1, 32), dtype=float32)', 'Tensor(\"Variable_33/read:0\", shape=(3, 3, 3, 32, 64), dtype=float32)', 'Tensor(\"Variable_34/read:0\", shape=(54080, 1024), dtype=float32)', 'Tensor(\"Variable_35/read:0\", shape=(1024, 2), dtype=float32)', 'Tensor(\"Variable_36/read:0\", shape=(32,), dtype=float32)', 'Tensor(\"Variable_37/read:0\", shape=(64,), dtype=float32)', 'Tensor(\"Variable_38/read:0\", shape=(1024,), dtype=float32)', 'Tensor(\"Variable_39/read:0\", shape=(2,), dtype=float32)'] and loss Tensor(\"Mean_2:0\", dtype=float32)."
     ]
    }
   ],
   "source": [
    "def train_neural_network(x):\n",
    "    \n",
    "    train_data = much_data[:-15]\n",
    "    validation_data = much_data[-15:]\n",
    "    \n",
    "    \n",
    "    prediction = convolutional_3d(x)\n",
    "    \n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(prediction, y))\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "    correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "    sess= tf.Session()\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in range(15):\n",
    "        \n",
    "        for data in train_data:\n",
    "            X = data[0]\n",
    "            Y=data[1]\n",
    "            sess.run(optimizer, feed_dict = {x: X, y:Y})\n",
    "        \n",
    "        train_accuracy = sess.run(accuracy, feed_dict={x:X, y: Y})\n",
    "        print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "    \n",
    "    print(sess.run(accuracy, feed_dict = {x:[i[0] for i in validation_data], y:[i[1] for i in validation_data]}))\n",
    "\n",
    "\n",
    "train_neural_network(x)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
